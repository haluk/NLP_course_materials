{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with TensorFlow for Sentiment Analysis\n",
    "\n",
    "In this homework, our task and dataset are the same as the previous homework; classifying movie reviews into positive and negative sentiment groups. \n",
    "\n",
    "Similar to the first homework, we will count the number of occurences of each lemmatized word in each document. We will use these counts as features characterizing the documents. These features will be fed into a neural network with two hidden layers. In preproecssing step, we use `spacy`, `nltk`, `contractions`, and `beautifulsoup4` libraries for performing these actions to get a clean corpus:\n",
    "\n",
    "- stripping html tags\n",
    "- removing urls\n",
    "- removing accented characters\n",
    "- expand contractions\n",
    "- remove tweets (mentions)\n",
    "- removes punctuations\n",
    "- removing stop words\n",
    "- converting all words to lowercase\n",
    "\n",
    "Textual data is generally modelled better using sequential models such as a recurrent neural network. However, [Google guidelines](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) note that bag-of-words (n-gram) models perform better or comparably to sequential models when a certain ratio exists in the dataset between the length of the documents and the number of words per document in the corpus. We will analyze this in [Section 1](#dataset_stats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections of The Notebook\n",
    "1. [Loading Dataset and Basic Statistics](#dataset_stats)\n",
    "2. [Counting Words](#word_counts)\n",
    "3. [Train/Test Set Summary After Preprocessing](#data_summary)\n",
    "4. [Building Neural Network Model](#model)\n",
    "5. [Exercise](#exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import preprocess  # preprocessing functions for cleaning text\n",
    "import utils  # read data, populate train/test sets\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataset_stats> </a>\n",
    "## 1. Loading Dataset and Basic Statistics\n",
    "\n",
    "We are reading in the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Refer to homework 1 for details about the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the value for this ratio is small (<1500), small multi-layer perceptrons that take n-grams as input (which we'll call Option A) perform better or at least as well as sequence models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path specification for the files\n",
    "ARCHIVE_NAME = \"./data/aclImdb_v1.tar.gz\"\n",
    "    \n",
    "with tarfile.open(ARCHIVE_NAME, \"r:gz\") as tar:\n",
    "    X_train, y_train, X_test, y_test = utils.get_raw_data_from(tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function below to plot the distribution of document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample_length_distribution(texts):\n",
    "    \"\"\"Plots the sample length distribution\n",
    "\n",
    "    :param texts: list of texts\n",
    "    :returns: None\n",
    "    :rtype: None\n",
    "\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in texts], 50)\n",
    "    plt.xlabel(\"Length of a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.title(\"Sample length distribution\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample length distribution for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_length_distribution(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample length distribution for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_length_distribution(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function to calculate the median number of words in documents below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(texts):\n",
    "    \"\"\"Returns the median number of words per sample\n",
    "    \n",
    "    :param texts: list of texts\n",
    "    :returns: median of number of words per sample\n",
    "    :rtype: int\n",
    "\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in texts]\n",
    "\n",
    "    return int(np.median(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:50s}: {}\".format(\"Number of words per sample in training (median)\", get_num_words_per_sample(X_train)))\n",
    "print(\"{:50s}: {}\".format(\"Number of words per sample in test (median)\", get_num_words_per_sample(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $25000$ samples in the training set and $174$ words per document. $25000/174 \\approx 144 < 1500$. According to the Google guideline, we can get good results using a fully connected neural network with word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=word_counts></a>\n",
    "## 2. Counting words\n",
    "\n",
    "We are tokenizing and getting the unigram counts. We will only keep the words that occur in at least 100 of the documents in the corpus. This will reduce the feature set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=utils.tokenizer, ngram_range=(1,1), min_df=100)\n",
    "%time X_train_cnt = vectorizer.fit_transform(X_train)\n",
    "%time X_test_cnt = vectorizer.transform(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=data_summary></a>\n",
    "## 3. Train/Test Set Summary After Preprocessing\n",
    "\n",
    "Here, we have a simple summary about train/test sets that can help us to understand the dataset and the type of classification problem that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:50s}: {}\".format(\"Number of training set samples\", X_train_cnt.shape[0]))\n",
    "print(\"{:50s}: {}\".format(\"Number of test set samples\", X_test_cnt.shape[0]))\n",
    "\n",
    "y_train_cnt = Counter(y_train)\n",
    "y_test_cnt = Counter(y_test)\n",
    "\n",
    "train_sample_cnt = \", \".join([f\"{k}:{y_train_cnt[k]}\" for k in sorted(y_train_cnt)])\n",
    "test_sample_cnt = \", \".join([f\"{k}:{y_test_cnt[k]}\" for k in sorted(y_test_cnt)])\n",
    "print(\"{:50s}: {:10s}\".format(\"Number of samples per class in training set\", train_sample_cnt))\n",
    "print(\"{:50s}: {:10s}\".format(\"Number of samples per class in test set\", test_sample_cnt))\n",
    "assert X_train_cnt.shape[1] == X_test_cnt.shape[1]\n",
    "print(\"{:50s}: {}\".format(\"Number of word in vocabulary\", X_train_cnt.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=model></a>\n",
    "## 4. Building Neural Network Model\n",
    "\n",
    "We are going to use fully connected neural network model. We will use sigmoid activation at the output layer since we only have two classes.\n",
    "\n",
    "TensorFlow allows [functional](https://www.tensorflow.org/guide/keras/functional) and [sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) API. We will use the sequential API. \n",
    "\n",
    "\n",
    "TensorFlow has many different [optimization algorithms](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). We will use Adam. To avoid overfitting, there are several regularization alternatives. We will use dropout strategy and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _form_last_layer(num_classes):\n",
    "    \"\"\"Forms the classification layer and sets activation function\n",
    "\n",
    "    :param num_classes: number of classes\n",
    "    :returns: units, activation\n",
    "    :rtype: tuple\n",
    "\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    elif num_classes > 2:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "    else:\n",
    "        raise Exception(\"Number of classes should be at least 2.\")\n",
    "        \n",
    "    return units, activation\n",
    "\n",
    "\n",
    "def build_mlp(num_layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"defines a fully connected neural network based on num_layers and units in each layer\n",
    "\n",
    "    :param num_layers: number of hidden layers\n",
    "    :param units: list of number of hidden units\n",
    "    :param dropout_rate: probability of dropping out unit\n",
    "    :param input_shape: input feature size in tuple\n",
    "    :param num_classes: number of classes\n",
    "    :returns: model\n",
    "    :rtype: tf.keras.models.Sequential\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    # for each layer\n",
    "    for i in range(num_layers - 1):\n",
    "\n",
    "        # add a fully connected layer\n",
    "        model.add(layers.Dense(units=units[i], activation=\"relu\"))\n",
    "        # with a dropout rate of `dropout_rate`\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # get the output layer units and activation function\n",
    "    o_units, o_activation = _form_last_layer(num_classes)\n",
    "\n",
    "    # and add it to the model\n",
    "    model.add(layers.Dense(units=o_units, activation=o_activation))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the number of distinct classes\n",
    "# from the training labels\n",
    "num_classes=len(set(y_train))\n",
    "\n",
    "# build the neural network model with \n",
    "# 1 input + 2 hidden + 1 output layer\n",
    "model = build_mlp(\n",
    "    \n",
    "    num_layers=3,\n",
    "    # hidden layers have 64 and 32 units\n",
    "    units=[64, 32],\n",
    "    \n",
    "    # each unit can be dropped out \n",
    "    # with a probability of 0.3\n",
    "    dropout_rate=0.3,\n",
    "    # input shape will be the size of the vocabulary\n",
    "    input_shape=X_train_cnt.shape[1:],\n",
    "    \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Compile model with learning parameters.\n",
    "if num_classes == 2:\n",
    "    # for two classes loss function is `binary_crossentropy`\n",
    "    loss = \"binary_crossentropy\"\n",
    "else:\n",
    "    # for more than two classes, we use `sparse_categorical_crossentropy`\n",
    "    loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "# we will report accuracy (`acc`) during training\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"acc\"])\n",
    "\n",
    "# early stopping will depend on the validation loss\n",
    "# patience parameter determines how many epochs with no improvement\n",
    "# in validation loss will be tolerated\n",
    "# before training is terminated.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_cnt.todense(),\n",
    "    y_train,\n",
    "    # keep 10% of the training data for validation\n",
    "    validation_split=0.1,\n",
    "    epochs=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,  # Logs once per epoch.\n",
    "    batch_size=128,\n",
    "    # Our neural network will be trained \n",
    "    # with stochastic (mini-batch) gradient descent. \n",
    "    # It is important that we shuffle our input.\n",
    "    shuffle=True # set to True by default\n",
    ")\n",
    "\n",
    "# Print training history\n",
    "history = history.history\n",
    "print(\n",
    "    \"\\nValidation accuracy: {acc}, loss: {loss}\".format(\n",
    "        acc=history[\"val_acc\"][-1], loss=history[\"val_loss\"][-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = (model.predict(X_test_cnt, batch_size=128) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise></a>\n",
    "## 5. Exercise\n",
    "\n",
    "#### 1. TF-IDF is a metric that reflects importance of words better than mere counts. Train the neural network above with [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Report model performance metrics on the test set.\n",
    "\n",
    "#### 2. Experiment with the number of hidden layers, number of units in the hidden layers and the dropout rate. Report model performance metrics on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "Please contact Haluk Dogan (<a href=\"mailto:hdogan@vivaldi.net\">hdogan@vivaldi.net</a>) for further questions or inquries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hw)",
   "language": "python",
   "name": "hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
