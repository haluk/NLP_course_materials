{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation with Attention\n",
    "\n",
    "In this notebook we will show you how to train a `seq2seq` model for English to\n",
    "Turkish translation. When you train the model, you will be able to translate\n",
    "English sentences to Turkish.\n",
    "\n",
    "We heavily borrowed from [TensorFlow's tutorial on Neural Machine Translation with Attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import Video\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import preprocess\n",
    "import utils\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections of the Notebook\n",
    "1. [Loading Dataset](#load)\n",
    "2. [Preparing Dataset](#prepare)\n",
    "3. [Seq2Seq Models](#seq2seq)\n",
    "4. [Training and Optimizer](#training)\n",
    "5. [Evaluation and Testing](#testing)\n",
    "6. [Exercises](#exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### 1. Loading Dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. They\n",
    "provide translation datasets for 80 different languages to/from English. The\n",
    "dataset is in tab separated tabular format with 3 columns. First column is a\n",
    "sentence in one of the 80 languages, and second is its translation in English.\n",
    "Third column shows the source of the row. We can ignore third column for our\n",
    "purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \"tur-eng.zip\", origin=\"https://github.com/haluk/NLP_course_materials/blob/master/hw4/tur-eng.zip?raw=true\",\n",
    "    extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/tur.txt\"\n",
    "\n",
    "num_examples = 1000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = utils.load_dataset(\n",
    "    path_to_file, num_examples\n",
    ")\n",
    "\n",
    "# we translate from English to Turkish\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = target_tensor, input_tensor, targ_lang, inp_lang\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "(\n",
    "    input_tensor_train,\n",
    "    input_tensor_val,\n",
    "    target_tensor_train,\n",
    "    target_tensor_val,\n",
    ") = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(\n",
    "    \"{:15s} => {:10s}: {}\\t{:15s}: {}\".format(\n",
    "        \"Input language\",\n",
    "        \"Training size\",\n",
    "        len(input_tensor_train),\n",
    "        \"Validation size\",\n",
    "        len(input_tensor_val),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"{:15s} => {:10s}: {}\\t{:15s}: {}\".format(\n",
    "        \"Target language\",\n",
    "        \"Training size\",\n",
    "        len(target_tensor_train),\n",
    "        \"Validation size\",\n",
    "        len(target_tensor_val),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show one sentence from input and target languages\n",
    "print(\"Input Language; index to word mapping\")\n",
    "utils.convert(inp_lang, input_tensor_train[30])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "utils.convert(targ_lang, target_tensor_train[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=prepare></a>\n",
    "### 2. Preparing Dataset \n",
    "We will use `tf.data.Dataset` API for building an asynchronous, highly optimized\n",
    "data pipeline to prevent GPUs from data starvation. It loads data from the disk,\n",
    "text in our case, creates batches and sends it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_tensor_train, target_tensor_train)\n",
    ").shuffle(BUFFER_SIZE)\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=seq2seq></a>\n",
    "### 3. Seq2Seq Models\n",
    "\n",
    "We will use [Jay Alammar's](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) wonderful visualizations to explain `seq2seq` model and `attention` mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" controls  width=\"900\"  height=\"200\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://jalammar.github.io/images/seq2seq_2.mp4\", width=900, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://jalammar.github.io/images/seq2seq_4.mp4\" controls  width=\"900\"  height=\"200\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://jalammar.github.io/images/seq2seq_4.mp4\", width=900, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Machine Translation (NMT) model is composed of an `encoder` and `decoder`. Encoder part of the model processes each token in the input sequence, and captures the learned information to a vector called `context` with size of given number of units. On the other hand, decoder part of the model gets `context` vector as input and produces output sequence token by token. In NMT model, both `encoder` and `decoder` are RNNs. We define `encoder` and `decoder` models in `models.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://jalammar.github.io/images/RNN_1.mp4\" controls  width=\"900\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://jalammar.github.io/images/RNN_1.mp4\", width=900, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrolled view of `RNN` based `encoder` and `decoder` processing steps. The `decoder` finds the relevant parts of the input for a given decoding step, first looks at the hidden states of the encoder and score them. Then, softmaxes the scores and multiplies the hidden states with these softmaxed scores. This results in the hidden states with high scores to be amplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" controls  width=\"900\"  height=\"300\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://jalammar.github.io/images/seq2seq_7.mp4\", width=900, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=training></a>\n",
    "### 4. Training and Optimizer\n",
    "\n",
    "   1. Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "   2. The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "   3. The decoder returns the predictions and the decoder hidden state.\n",
    "   4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "   5. Use teacher forcing to decide the next input to the decoder.\n",
    "   6. Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "   7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss        \n",
    "        if batch % 100 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Batch {} Loss {:.4f}\".format(\n",
    "                    epoch + 1, batch, batch_loss.numpy()\n",
    "                )\n",
    "            )\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print(\"Time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=testing></a>\n",
    "### 5. Evaluation and Testing\n",
    "\n",
    "Evaluation is similar to training except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess.preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(\" \")]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs], maxlen=max_length_inp, padding=\"post\"\n",
    "    )\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            dec_input, dec_hidden, enc_out\n",
    "        )\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + \" \"\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == \"<end>\":\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "\n",
    "    fontdict = {\"fontsize\": 14}\n",
    "\n",
    "    ax.set_xticklabels([\"\"] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print(\"Input: %s\" % (sentence))\n",
    "    print(\"Predicted translation: {}\".format(result))\n",
    "\n",
    "    attention_plot = attention_plot[\n",
    "        : len(result.split(\" \")), : len(sentence.split(\" \"))\n",
    "    ]\n",
    "    plot_attention(attention_plot, sentence.split(\" \"), result.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the wrapper function, `translate`, we translate the given sentence in input language to target language and plot attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Tell me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise></a>\n",
    "### 6. Exercises\n",
    "\n",
    "\n",
    "1. Due to computational limitations inside the Jupyter container, we used a size 1,000 subset of the training set. Train the model for at least 10 epochs using all training examples. (*Hint*: If you set `num_examples` to None, you will use the whole dataset.) \n",
    "\n",
    "2. Plot the attention weights for the new model. In Jupyter, we can see the plots in the notebook, however, when you use batch submission, you need to save the plots to a file. You will need [this API](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.savefig.html) instead of `plt.show()`.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "Please contact Haluk Dogan (<a href=\"mailto:hdogan@vivaldi.net\">hdogan@vivaldi.net</a>) for further questions or inquries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
