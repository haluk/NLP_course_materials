{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build a question answering (QA) deep learning architecture. To fully utilize the available context information, we will make use of BiLSTM units. Instead of one-hot encoding words, we will use GloVe embeddings to make lexical semantics available to the model. We follow the architecture shown in the figure below copied from our [textbook](https://web.stanford.edu/~jurafsky/slp3/25.pdf) (Fig 25.7). We differ in the representation of the context: we only use GloVe embeddings while the architecture in the textbook uses an enriched representation with POS, NER tags, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QA-Deep-Learning-Architecture](img/arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import arch_utils\n",
    "import utils\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturesDict({\n",
      "    'answers': Sequence({\n",
      "        'answer_start': tf.int32,\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    'context': Text(shape=(), dtype=tf.string),\n",
      "    'id': tf.string,\n",
      "    'question': Text(shape=(), dtype=tf.string),\n",
      "    'title': Text(shape=(), dtype=tf.string),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "CWD = Path.cwd().as_posix()\n",
    "\n",
    "squad_data, info = tfds.load(\"squad\", data_dir=CWD, with_info=True)\n",
    "squad_train = squad_data[\"train\"]\n",
    "squad_validation = squad_data[\"validation\"]\n",
    "print(info.features)\n",
    "context_tr, question_tr, answer_text_tr, answer_start_tr = utils.split_info(squad_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "start_char_idx = answer_start_tr[i]\n",
    "print(context_tr[i][0:start_char_idx-1])\n",
    "print()\n",
    "print(context_tr[i][0:start_char_idx-1].replace(\"-\", \" \"))\n",
    "print()\n",
    "\n",
    "print(\"Context: \", context_tr[i], \"\\n\")\n",
    "print(\"Question: \", question_tr[i], \"\\n\")\n",
    "print(\"Answer: \", answer_text_tr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_type = \"post\"\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(context_tr)\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index.keys())\n",
    "\n",
    "print(\"{:50s}: {}\".format(\"Total number of words\", num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding context vectors to integer vectors and post padding them \n",
    "sequences = tokenizer.texts_to_sequences(context_tr)\n",
    "context_len = max(map(len, sequences))\n",
    "print(\"{:50s}: {}\".format(\"Max length of a context vector\", context_len))\n",
    "context_padded = pad_sequences(sequences, maxlen=context_len, padding=padding_type)\n",
    "\n",
    "# encoding question vectors to integer vectors and post padding them \n",
    "sequences = tokenizer.texts_to_sequences(question_tr)\n",
    "question_len = max(map(len, sequences))\n",
    "print(\"{:50s}: {}\".format(\"Max length of a question vector\", question_len))\n",
    "question_padded = pad_sequences(sequences, maxlen=question_len, padding=padding_type)\n",
    "\n",
    "# encoding answer vectors into integer vectors\n",
    "answer_token = tokenizer.texts_to_sequences(answer_text_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tup = []  # (start,end)\n",
    "selected = []  # we keep context that only contains exact answer\n",
    "WINDOW = 10\n",
    "\n",
    "for i in range(len(answer_start_tr)):\n",
    "    start_char_idx = answer_start_tr[i]\n",
    "    start = len(context_tr[i][0 : start_char_idx - 1].replace(\"-\", \" \").split()) + 1\n",
    "    answer_len = len(answer_text_tr[i].replace(\"-\", \" \").split())\n",
    "    end = start + answer_len\n",
    "\n",
    "    for j in range(start - WINDOW, start + WINDOW):\n",
    "        if np.array_equal(context_padded[i][j : j + answer_len], answer_token[i]):\n",
    "            start = j\n",
    "            end = j + answer_len - 1\n",
    "            y_train_tup.append((start, end))\n",
    "            selected.append(i)\n",
    "            break\n",
    "\n",
    "context_padded_clean = context_padded[selected]\n",
    "question_padded_clean = question_padded[selected]\n",
    "answer_text_clean = answer_text_tr[selected]\n",
    "\n",
    "num_train_data = context_padded_clean.shape[0]\n",
    "print(\"{:50s}: {}\".format(\"Number of training samples after cleaning\", num_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(context_padded_clean)):\n",
    "    s = np.zeros(context_len, dtype=\"float32\")\n",
    "    e = np.zeros(context_len, dtype=\"float32\")\n",
    "\n",
    "    s[y_train_tup[i][0]] = 1\n",
    "    e[y_train_tup[i][1]] = 1\n",
    "\n",
    "    y_train.append(np.concatenate((s, e)))\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head glove.6B.50d.txt | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50 # glove.6B.50d\n",
    "embeddings_mat = arch_utils.load_embeddings(\"glove.6B.50d.txt\", num_words, emb_dim, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 128\n",
    "context_input = Input(shape=(context_len,))\n",
    "context_emb = Embedding(num_words, embeddings_mat, emb_dim)(context_input)\n",
    "context_lstm = BiLSTM(units)(context_emb)\n",
    "\n",
    "question_input = Input(shape=(question_len,))\n",
    "question_emb = Embedding(num_words, embeddings_mat, emb_dim)(question_input)\n",
    "question_lstm = BiLSTM(units)(question_emb)\n",
    "\n",
    "y_prob = BiLinear_Layer(2 * units, question_len)(context_lstm, question_lstm)\n",
    "\n",
    "model = Model(inputs=[context_input, question_input], outputs=y_prob)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=arch_utils.loss,\n",
    "    metrics=[arch_utils.Custom_Accuracy(num_train_data)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_padded_jupyter = context_padded_clean[:1000]\n",
    "question_padded_jupyter = question_padded_clean[:1000]\n",
    "y_train_jupyter = y_train[:1000]\n",
    "\n",
    "# model.load_weights(\"epochs_1000\")\n",
    "init_epoch = 0\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "# early stopping will depend on the validation loss\n",
    "# patience parameter determines how many epochs with no improvement\n",
    "# in validation loss will be tolerated\n",
    "# before training is terminated.\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "\n",
    "filepath = \"epochs_{epoch:03d}\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True)\n",
    "\n",
    "callbacks = [earlystopping, checkpoint]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[context_padded_jupyter, question_padded_jupyter],\n",
    "    y=y_train_jupyter,\n",
    "    # keep 10% of the training data for validation\n",
    "    validation_split=0.1,\n",
    "    initial_epoch=init_epoch,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,  # Logs once per epoch.\n",
    "    batch_size=batch_size,\n",
    "    # Our neural network will be trained\n",
    "    # with stochastic (mini-batch) gradient descent.\n",
    "    # It is important that we shuffle our input.\n",
    "    shuffle=True,  # set to True by default\n",
    ")\n",
    "\n",
    "# Print training history\n",
    "history = history.history\n",
    "print(\n",
    "    \"\\nValidation accuracy: {acc}, loss: {loss}\".format(\n",
    "        acc=history[\"val_custom_accuracy\"][-1], loss=history[\"val_loss\"][-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_padded_test_jupyter = context_padded_clean[1000:2000]\n",
    "question_padded_test_jupyter = question_padded_clean[1000:2000]\n",
    "y_test_jupyter = y_train[1000:2000]\n",
    "\n",
    "model.evaluate(\n",
    "    [context_padded_test_jupyter, question_padded_test_jupyter],\n",
    "    y_test_jupyter,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
